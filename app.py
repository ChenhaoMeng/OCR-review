import streamlit as st
import pandas as pd
import json
import base64
import fitz  # PyMuPDF
from PIL import Image
import io
from openai import OpenAI

# --- å…¨å±€é…ç½® ---
# 1. å°è¯•ä» Streamlit Secrets è¯»å– Key (ç”Ÿäº§ç¯å¢ƒ)
# 2. å¦‚æœæ²¡æœ‰ï¼Œåˆ™ç•™ç©º (ä¼šæŠ¥é”™æç¤º)
try:
    API_KEY = st.secrets["SJTU_API_KEY"]
except Exception:
    API_KEY = "" # æœ¬åœ°æµ‹è¯•æ—¶ï¼Œå¦‚æœæ²¡é…ç½® .streamlit/secrets.toml ä¼šèµ°è¿™é‡Œ

API_BASE = "https://models.sjtu.edu.cn/api/v1"

st.set_page_config(page_title="å¾·è¯­æ•™æ AI è§£æå™¨", layout="wide")

# ... (å…¶ä½™ä»£ç ä¿æŒä¸å˜) ...

# åœ¨è°ƒç”¨ API ä¹‹å‰å¢åŠ ä¸€ä¸ªæ£€æŸ¥
if not API_KEY:
    st.error("æœªæ£€æµ‹åˆ° API Keyã€‚è¯·åœ¨ Streamlit Cloud Secrets ä¸­é…ç½® 'SJTU_API_KEY'ã€‚")
    st.stop()

# ... (ç¡®ä¿ extract_text_with_vision å’Œ analyze_grammar å‡½æ•°é‡Œä½¿ç”¨çš„æ˜¯è¿™ä¸ª API_KEY) ...


# --- 1. è¾…åŠ©å‡½æ•°ï¼šPDF é¡µè½¬ Base64 å›¾ç‰‡ ---
def pdf_page_to_base64(uploaded_file, page_number=0):
    """
    å°†ä¸Šä¼ çš„ PDF æ–‡ä»¶çš„æŒ‡å®šé¡µé¢è½¬æ¢ä¸º Base64 ç¼–ç çš„å›¾åƒå­—ç¬¦ä¸²ã€‚
    è¿™æ ·å¯ä»¥ç›´æ¥å‘é€ç»™ APIã€‚
    """
    # ä½¿ç”¨ PyMuPDF æ‰“å¼€æ–‡ä»¶æµ
    doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
    
    if page_number >= len(doc):
        return None, "é¡µç è¶…å‡ºèŒƒå›´"
        
    page = doc.load_page(page_number)
    
    # å°†é¡µé¢æ¸²æŸ“ä¸ºåƒç´ å›¾ (dpi=150 ä¿è¯æ¸…æ™°åº¦ä¸”ä¸ä¼šå¯¼è‡´å›¾ç‰‡è¿‡å¤§æ¶ˆè€—å¤ªå¤š Token)
    pix = page.get_pixmap(dpi=150)
    
    # è½¬æ¢ä¸º PIL Image
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    
    # è½¬æ¢ä¸º Base64
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    
    return img_str, img

# --- 2. AI è§†è§‰æå–æ–‡æœ¬ (OCR) ---
def extract_text_with_vision(base64_image):
    """
    ä½¿ç”¨ Qwen3-VL-32B æ¨¡å‹ä»å›¾ç‰‡ä¸­æå–å¾·è¯­æ–‡æœ¬ã€‚
    """
    client = OpenAI(base_url=API_BASE, api_key=API_KEY)
    
    try:
        response = client.chat.completions.create(
            model="Qwen3-VL-32B",  # ä½¿ç”¨è§†è§‰æ¨¡å‹
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text", 
                            "text": "è¯·å°†è¿™å¼ å›¾ç‰‡ä¸­çš„å¾·è¯­æ–‡æœ¬å®Œæ•´åœ°è½¬å½•å‡ºæ¥ã€‚åªè¾“å‡ºå¾·è¯­å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚å¦‚æœåŒ…å«æ ‡é¢˜å’Œæ­£æ–‡ï¼Œè¯·æŒ‰é¡ºåºè½¬å½•ã€‚"
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=1000  # é™åˆ¶è¾“å‡ºé•¿åº¦
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"

# --- 3. AI è¯­æ³•åˆ†æ (çº¯æ–‡æœ¬å¤„ç†) ---
@st.cache_data(show_spinner=False)
def analyze_grammar(text):
    """
    ä½¿ç”¨ DeepSeek-V3 åˆ†ææå–å‡ºæ¥çš„æ–‡æœ¬
    """
    client = OpenAI(base_url=API_BASE, api_key=API_KEY)
    
    # æˆªæ–­æ–‡æœ¬ä»¥é˜²æ­¢è¶…è¿‡ Token é™åˆ¶ (æ¯å‘¨/æ¯åˆ†é’Ÿé™åˆ¶)
    # å‡è®¾æ¯åˆ†é’Ÿåªèƒ½è·‘ 3000 tokenï¼Œæˆ‘ä»¬è¿™é‡Œå°½é‡ä¿å®ˆ
    safe_text = text[:800] 
    
    prompt = f"""
    åˆ†æä»¥ä¸‹å¾·è¯­æ–‡æœ¬ã€‚æå–é‡ç‚¹å•è¯ï¼ˆè¿‡æ»¤ç®€å•ä»‹è¯å’Œå† è¯ï¼‰ã€‚
    è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›åˆ—è¡¨ï¼Œä¸è¦ä½¿ç”¨ Markdown æ ¼å¼ã€‚
    å­—æ®µ: word(åŸè¯), pos(è¯æ€§), meaning(ä¸­æ–‡), usage(è¯­æ³•/æ­é…), example(æçŸ­ä¾‹å¥)ã€‚
    
    æ–‡æœ¬: "{safe_text}"
    """

    try:
        response = client.chat.completions.create(
            model="DeepSeek-V3-685B", # æ–‡æœ¬åˆ†æèƒ½åŠ›æ›´å¼º
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè¾“å‡ºçº¯ JSON çš„å¾·è¯­åŠ©æ•™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=1500
        )
        content = response.choices[0].message.content
        clean_content = content.replace("```json", "").replace("```", "").strip()
        return json.loads(clean_content)
    except Exception as e:
        return {"error": str(e)}

# --- 4. ç•Œé¢é€»è¾‘ ---
st.title("ğŸ‡©ğŸ‡ª å¾·è¯­æ•™æ OCR æ™ºèƒ½åŠ©æ‰‹")
st.caption("æ”¯æŒ PDF æ‰«æä»¶ï¼šåˆ©ç”¨ Qwen3-VL è¯†å›¾ -> DeepSeek-V3 åˆ†æ")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("æ–‡ä»¶ä¸Šä¼ ")
    uploaded_file = st.file_uploader("ä¸Šä¼ æ•™æ PDF", type=["pdf"])
    page_num = st.number_input("é€‰æ‹©é¡µç  (ä»0å¼€å§‹)", min_value=0, value=0, step=1)
    
    st.divider()
    st.warning("âš ï¸ èµ„æºé™åˆ¶æç¤ºï¼š\næ¯åˆ†é’Ÿé™åˆ¶ 3000 Tokensã€‚\nå»ºè®®æ¯æ¬¡åªåˆ†æä¸€é¡µï¼Œæ“ä½œé—´éš” 30 ç§’ä»¥ä¸Šã€‚")

if uploaded_file is not None:
    # 1. è½¬æ¢å›¾ç‰‡
    with st.spinner("æ­£åœ¨æ¸²æŸ“ PDF é¡µé¢..."):
        # å¿…é¡»é‡ç½®æ–‡ä»¶æŒ‡é’ˆï¼Œå¦åˆ™åˆ‡æ¢é¡µç æ—¶ä¼šæŠ¥é”™
        uploaded_file.seek(0) 
        base64_img, pil_img = pdf_page_to_base64(uploaded_file, page_num)
    
    if pil_img:
        # å±•ç¤ºå›¾ç‰‡
        st.image(pil_img, caption=f"ç¬¬ {page_num} é¡µé¢„è§ˆ", use_container_width=True)
        
        # æŒ‰é’®è§¦å‘ OCR å’Œåˆ†æ
        if st.button("ğŸ” æå–æ–‡å­—å¹¶åˆ†æè¯­æ³•", type="primary"):
            
            # 2. è§†è§‰æå– (è€—è´¹ Token)
            with st.spinner("æ­£åœ¨ä½¿ç”¨ Qwen3-VL è¯»å–å›¾ç‰‡æ–‡å­—..."):
                extracted_text = extract_text_with_vision(base64_img)
            
            if "Error" in extracted_text:
                st.error("å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚")
                st.error(extracted_text)
            else:
                st.subheader("ğŸ“„ æå–çš„æ–‡æœ¬")
                st.text_area("OCR ç»“æœ (å¯æ‰‹åŠ¨ä¿®æ­£)", value=extracted_text, height=150, key="ocr_text")
                
                # 3. è¯­æ³•åˆ†æ (è€—è´¹ Token)
                # ä½¿ç”¨ session_state ä¸­çš„å€¼ï¼Œå…è®¸ç”¨æˆ·ä¿®æ­£ OCR é”™è¯¯åå†åˆ†æ
                text_to_analyze = st.session_state.ocr_text if "ocr_text" in st.session_state else extracted_text
                
                with st.spinner("æ­£åœ¨ä½¿ç”¨ DeepSeek-V3 è§£æè¯­æ³•..."):
                    analysis_result = analyze_grammar(text_to_analyze)
                
                if "error" in analysis_result:
                    st.error(f"åˆ†æå‡ºé”™: {analysis_result['error']}")
                else:
                    st.subheader("ğŸ“ è¯­æ³•è¯¦è§£")
                    df = pd.DataFrame(analysis_result)
                    st.dataframe(
                        df, 
                        column_config={
                            "word": "å•è¯", "pos": "è¯æ€§", "meaning": "ä¸­æ–‡å«ä¹‰", 
                            "usage": "ç”¨æ³•/æ­é…", "example": "ä¾‹å¥"
                        },
                        use_container_width=True,
                        hide_index=True
                    )
    else:
        st.error("æ— æ³•åŠ è½½è¯¥é¡µé¢ï¼Œå¯èƒ½é¡µç è¶…å‡ºäº†æ–‡ä»¶èŒƒå›´ã€‚")
else:
    st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF æ–‡ä»¶ã€‚")
